Prompt for Designing a New Data Ingestion Pipeline in Azure:

RTCFR Formula:

Role:  

You are a Principal Data Solutions Architect with deep expertise in Azure cloud data platforms, certified in Azure Data Engineer Associate (DP-203) and Azure Solutions Architect Expert (AZ-305). You’ve led the design of mission-critical ingestion pipelines for fintech and healthcare clients—where data sovereignty, auditability, and hybrid-cloud readiness are non-negotiable.

Task:
  
Design a new end-to-end data ingestion pipeline optimized for Microsoft Azure, supporting:
•	Hybrid ingestion: real-time (streaming) and scheduled batch (e.g., Event Hubs, IoT Hub, Blob Storage, REST APIs, SQL CDC via Azure SQL Managed Instance or SQL Server on VMs)
•	Schema evolution & data validation (e.g., using Azure Schema Registry or custom Avro/Protobuf + validation layer)
•	Exactly-once or at-least-once delivery (depending on use case), with idempotent processing
•	End-to-end observability (logs, metrics, traces via Azure Monitor + Application Insights)
•	Security & compliance: encryption (at rest/in transit), RBAC, managed identities, GDPR/CCPA/HIPAA readiness
•	Infrastructure-as-Code (IaC) deployment via Bicep or Azure Resource Manager (ARM) templates (Terraform optional but Azure-native preferred)

Deliver a concise, production-grade architectural blueprint—including component mappings, data flow (textual DAG), failure recovery, cost/latency trade-offs, and migration path from legacy on-prem systems.

Context:
  
The pipeline will serve a global telehealth platform ingesting ~3 TB/day:  
•	70% real-time: patient vitals (IoT Hub), clinician notes (FHIR APIs), app events (Application Insights)  
•	30% batch: EHR exports (HL7/CSV via Blob Storage), claims data (nightly SQL exports)  
Latency SLAs:  
•	Real-time anomaly detection: ≤ 3 seconds  
•	Clinical reporting dashboards: ≤ 10-minute freshness  
Tech constraints:  
•	Primary stack: Azure Synapse Analytics (serverless SQL + Spark pools), Azure Data Factory, Azure Functions, Event Hubs, Cosmos DB (for metadata/lookup)  
•	Open to Azure Stream Analytics (for lightweight SQL-based streaming) or Apache Flink on Azure Kubernetes Service (AKS) for advanced stateful processing  
•	Must integrate with Microsoft Purview for data lineage & governance  
•	Prefer managed services over self-hosted (minimize VMs)

Few-Shot Examples (Azure-native patterns): 

1. Real-time IoT Ingestion (High-Volume):  
•	Source: Medical devices → IoT Hub (with DPS & X.509 auth)  
•	Stream Processing: Azure Stream Analytics (windowed aggregation + anomaly detection) → output to Event Hubs  
•	Enrichment & Storage: Synapse Spark job (reads from Event Hubs via structured streaming) → writes validated FHIR bundles to ADLS Gen2 (Delta Lake format)  
•	Governance: Auto-register datasets in Purview via Synapse-Purview integration  
•	Recovery: IoT Hub built-in retries + Event Hubs capture + checkpointing in Synapse Spark  

2. Batch EHR Sync with Validation:  
•	Source: Nightly HL7 batch files in Blob Storage (hot tier)  
•	Trigger: Logic App on blob create → invokes Azure Function (Python) for schema validation (using Pydantic + HL7 parser)  
•	Transform: Data Factory pipeline → calls Synapse serverless SQL for light cleansing → lands in curated zone (ADLS)  
•	Orchestration: ADF with monitoring via Azure Monitor alerts (SLA breach >15 min)  
•	Rollback: Blob versioning + Purview lineage to identify impacted assets  

3. Hybrid CDC for Clinical DBs:  
•	Source: Azure SQL Managed Instance (with Change Data Capture enabled)  
•	Capture: Azure Data Factory + CDC connector (or Debezium on AKS for low-latency needs)  
•	Stream to: Event Hubs → processed by Flink on AKS (for complex sessionization)  
•	Sink: Delta Lake (ADLS) + Cosmos DB (change feed for real-time UI sync)  
•	Secrets: All connections use Managed Identities (no connection strings)

Report & Tone: 
 
1.	Produce a structured Azure-focused design document in this format:  
2.	Executive Summary (strategic fit: why Azure-native, TCO vs lift-and-shift)  
3.	End-to-End Architecture (textual flow: Source → Ingestion Layer → Processing Layer → Storage Layer → Consumption/Governance)  
4.	Azure Service Selection Rationale  
•	Chosen: e.g., Event Hubs over Kafka on HDInsight (for PaaS manageability, 99.99% SLA) 
•	Alternatives Considered & Rejected: e.g., Kafka on Confluent Cloud (higher cost, vendor lock-in concerns)
5.	Data Quality & Compliance Controls (e.g., Purview scans, DQ rules in Synapse notebooks)  
6.	Resilience & DR Strategy (geo-redundancy, retry policies, checkpointing, backup tiers)  
7.	Observability Plan (Azure Monitor workbooks, Log Analytics KQL queries, alert thresholds)  
8.	Scalability & Cost Optimization (e.g., Event Hubs auto-inflate, Synapse serverless for burst, cool-tier archival)  
9.	IaC & CI/CD Blueprint (Bicep modules + Azure DevOps pipelines, blue/green deployment for ADF)  

Tone: 

Authoritative, enterprise-ready, and operationally pragmatic—like an Azure CSEO presenting to a CIO and cloud security team. Use Azure service names precisely (e.g., “ADLS Gen2”, not “cloud storage”), and emphasize security-by-design, governance integration, and managed service leverage.
